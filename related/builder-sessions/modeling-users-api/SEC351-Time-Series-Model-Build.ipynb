{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC351 - re:Invent 2019 Timeseries Forecasting - Example\n",
    "## _You should complete the data exploration notebook first_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries that we will need for the model training and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import important libraries\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "import boto3\n",
    "# import io\n",
    "# import re\n",
    "import sagemaker as sage\n",
    "# from time import gmtime, strftime\n",
    "# import itertools\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from io import StringIO\n",
    "# from io import BytesIO\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the datset locally and load into the s3 bucket\n",
    "_This steps assumes you've completed the accompanying data exploration notebook and have created an appropriate csv data file in the local directory_ In our case, our exploration generated \"AwsSecurityAudit_Meta31_DescribeDBInstances.csv\"   \n",
    "We first load the data set into a Pandas DataFrame for feature selection and cleaning. We select those features that are relevant to this forecasting task by manually specifying column names, parse the time, and\n",
    "- mark all NA values with 0\n",
    "- drop the first 24 hours\n",
    "\n",
    "We included optional code to save the data in a CSV file in the local notebook instance. Finally display the first 5 rows to inspect the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are reading the csv file that we created in the prior data exploration notebook\n",
    "def parse(x):\n",
    "    return datetime.strptime(x, '%Y %m %d %H')\n",
    "\n",
    "UserEventFreq_df = pd.read_csv('AwsSecurityAudit_Meta31_DescribeDBInstances.csv'\n",
    "                   , index_col=0)\n",
    "UserEventFreq_df.index.name = 'date'\n",
    "UserEventFreq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateFreq_df = UserEventFreq_df[['count']]\n",
    "dateFreq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateFreq_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store data in S3 \n",
    "A more persistent data store.  Just in case we close the notebook instance or want to use this data later.  \n",
    "To upload the data to S3, we define the name of the bucket and the prefix that will be used throughout the notebook. We then create an S3 resource client and upload the data to S3 directly from teh data frame object in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the s3 bucket\n",
    "region = boto3.Session().region_name\n",
    "bucket ='reinvent2019-builder-working' # <==  Change the name of this bucket\n",
    "prefix = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write data to s3 (rather than keeping in the notebook instance)\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "dateFreq_df.to_csv(csv_buffer)\n",
    "s3_resource.Object(bucket, prefix+'/data.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data\n",
    "Plot the features in the dataset to observe repeating pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_of_interest = ['count']\n",
    "pyplot.figure(figsize=(12,3*len(features_of_interest)))\n",
    "for i,f in enumerate(features_of_interest):\n",
    "    if i==0: ax0 = pyplot.subplot(len(features_of_interest), 1, i+1)\n",
    "    else: pyplot.subplot(len(features_of_interest), 1, i+1, sharex = ax0)\n",
    "    dateFreq_df[f].plot()\n",
    "    pyplot.title(f, y=0.85, loc='right')\n",
    "pyplot.subplots_adjust(hspace=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for timeseries forecasting\n",
    "We next define a function that will take timeseries data and create a data structure where input sequence `(t-n, ... t-1)` forecasts an output sequence `(t, t+1, ... t+n)`. This will provide training data for the forecasting algorithm.  \n",
    "\n",
    "In our simple example, we're just going to use the value at time t to predict the value at time t+1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    \n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)  # drop rows with NaN values\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series.values\n",
    "    \n",
    "    # integer encode direction\n",
    "    encoder = LabelEncoder()\n",
    "    raw_values[:,0] = encoder.fit_transform(raw_values[:,0])\n",
    "\n",
    "    # rescale values to 0, 1\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_values = scaler.fit_transform(raw_values)\n",
    "    scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "    \n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "    supervised = supervised[['var1(t-1)',\n",
    "                             'var1(t)']]\n",
    "    supervised_values = supervised.values\n",
    "    # split into train and test sets\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    return scaler, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeSeries = dateFreq_df\n",
    "\n",
    "# configure\n",
    "n_lag = 1\n",
    "n_seq = 1\n",
    "n_test = 30\n",
    "\n",
    "# prepare data\n",
    "scaler, train, test = prepare_data(timeSeries, n_test, n_lag, n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and testing sets and save in s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train)\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first 70 observations and save training set in s3:\n",
    "csv_buffer = StringIO()\n",
    "train_df[:70].to_csv(csv_buffer,index=False)\n",
    "s3_resource.Object(bucket, prefix+'/train/train.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save validation set in s3\n",
    "csv_buffer = StringIO()\n",
    "train_df[70:].to_csv(csv_buffer,index=False)\n",
    "s3_resource.Object(bucket, prefix+'/validation/val.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the docker image and push it to ECS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Set the name of our algorithm\n",
    "algorithm_name=reinvent2019-user-event-predict\n",
    "\n",
    "chmod +x train/train\n",
    "chmod +x train/serve\n",
    "\n",
    "# Get the account and region defined in the current configuration (default to us-west-2 if none defined)\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "sudo docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_df = pd.read_csv('s3://'+bucket+'/'+prefix+'/train/train.csv')\n",
    "traindata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and set up the endpoint for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sage.get_execution_role()\n",
    "sess = sage.Session()\n",
    "source_bucket_uri = 's3://'+bucket+'/'+prefix+'/train/'\n",
    "\n",
    "# Get account and region to create the image \n",
    "# Make sure we're suing the same algorithm name as above\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, 'reinvent2019-user-event-predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up SageMaker Estimator and fit the training job\n",
    "model = sage.estimator.Estimator(image,\n",
    "                      role, 1, 'ml.p3.8xlarge',\n",
    "                      output_path=\"s3://{}/output\".format(sess.default_bucket()),\n",
    "                      sagemaker_session=sess)\n",
    "model.fit(source_bucket_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model endpoint \n",
    "predictor = model.deploy(1, 'ml.t2.medium', serializer=sage.predictor.csv_serializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test that the endpoint is working and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData_df = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData_df=testData_df.iloc[:,:1]\n",
    "testData_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside - Testing the endpoint outside the notebook\n",
    "In the previous (and following) code, we call the model endpoint inference from within the notebook environment.  The more common use-case, however, is calling the endpoint in some account-local application.  For this, we'll use the _invoke__endpoint()_ API call. For example, we might call it from a lambda.  \n",
    "So, before we proceed, we just want to check that that API call works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Our simpfile.csv is just a file with two numbers - invoke_endpoing for this model\n",
    "#   likes file objects to read\n",
    "payload = pd.read_csv('simpfile.csv')\n",
    "payload_file = io.StringIO()\n",
    "payload.to_csv(payload_file, header = None, index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Need to set the EndpointName**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX', \n",
    "    ContentType = 'text/csv',\n",
    "    Body = payload_file.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of aside...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing and scoring unseen data\n",
    "Let's return to testing our new time series predictor, and see how we could score new points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictor.predict(testData_df.values).decode('utf-8').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1\n",
    "actual = [row[n_features:] for row in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = list()\n",
    "for i in range(len(test)):\n",
    "    # make forecast\n",
    "    forecast = [float(s) for s in predictions[i].split(',')]\n",
    "    # store the forecast\n",
    "    forecasts.append(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate RMSE in scaled and unscaled features space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the RMSE for each forecast time step\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
    "    for i in range(n_seq):\n",
    "        actual = [row[i] for row in test]\n",
    "        predicted = [forecast[i] for forecast in forecasts]\n",
    "        rmse = sqrt(mean_squared_error(actual, predicted))\n",
    "        print('t+%d RMSE: %f' % ((i+1), rmse))\n",
    "        \n",
    "evaluate_forecasts(actual, forecasts, n_lag, n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform back to the original scale\n",
    "raw_values = timeSeries.values\n",
    "\n",
    "# transform into supervised learning problem X, y\n",
    "supervised = series_to_supervised(raw_values, n_lag, n_seq)\n",
    "supervised = supervised[['var1(t)']]\n",
    "supervised_values = supervised.values\n",
    "\n",
    "# rescale values back to the original values\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_values = scaler.fit_transform(supervised_values)\n",
    "scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "\n",
    "inv_forecast = scaler.inverse_transform(forecasts)\n",
    "inv_actual = scaler.inverse_transform(actual)\n",
    "\n",
    "evaluate_forecasts(inv_actual, inv_forecast, n_lag, n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the forecasts in the context of the original dataset\n",
    "def plot_forecasts(series, forecasts, n_test):\n",
    "    # plot the entire dataset in blue\n",
    "    pyplot.figure(figsize=(20,5))\n",
    "    pyplot.plot(series.values)\n",
    "    # plot the forecasts in red\n",
    "    for i in range(len(forecasts)):\n",
    "        off_s = len(series) - n_test + i -1\n",
    "        off_e = off_s + len(forecasts[i]) +1\n",
    "        xaxis = [x for x in range(off_s, off_e)]\n",
    "        yaxis = [series.values[off_s]] + list(forecasts[i]) \n",
    "        pyplot.plot(xaxis, yaxis, color='red')\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polution = dateFreq_df['count'][-32:]\n",
    "polution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph of original series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_of_interest = ['count']\n",
    "pyplot.figure(figsize=(12,3*len(features_of_interest)))\n",
    "for i,f in enumerate(features_of_interest):\n",
    "    if i==0:\n",
    "        ax0 = pyplot.subplot(len(features_of_interest), 1, i+1)\n",
    "    else:\n",
    "        pyplot.subplot(len(features_of_interest), 1, i+1, sharex = ax0)\n",
    "    dateFreq_df[f].plot()\n",
    "    pyplot.title(f, y=0.85, loc='right')\n",
    "pyplot.subplots_adjust(hspace=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph of forecast on last 30 days of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecasts(polution, inv_forecast, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph errors to determine if our ongoing series has an anomoly. Higher than normal error = anomoly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = abs(inv_actual - inv_forecast)\n",
    "pyplot.figure(figsize=(20,5))\n",
    "pyplot.plot(errors)\n",
    "pyplot.title('Errors', y=0.85, loc='right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "errors = abs(inv_actual - inv_forecast)\n",
    "mean_error = np.mean(errors)\n",
    "pct_error = errors / mean_error * 100\n",
    "std_error = np.std(errors)\n",
    "pyplot.figure(figsize=(20,5))\n",
    "pyplot.plot(pct_error)\n",
    "pyplot.title('Percent Errors', y=0.85, loc='right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoint so you avoid any recuring charges\n",
    "sess.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
