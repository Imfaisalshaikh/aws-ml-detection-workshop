{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using IP Insights to score security findings\n",
    "-------\n",
    "Amazon SageMaker IP Insights is an unsupervised anomaly detection algorithm for susicipous IP addresses that uses statistical modeling and neural networks to capture associations between online resources (such as account IDs or hostnames) and IPv4 addresses. Under the hood, it learns vector representations for online resources and IP addresses.  \n",
    "  \n",
    "As a result, if the vector representing an IP address and an online resource are close together, then it is likely (not surprising) for that IP address to access that online resource, even if it has never accessed it before.\n",
    "\n",
    "In this notebook, we use the Amazon SageMaker IP Insights algorithm to train a model using the `<principal ID, IP address`> tuples we generated from the CloudTrail log data, and then use the model to perform inference on the same type of tuples generated from GuardDuty findings to determine how unusual it is to see a particular IP address for a given principal involved with a finding.\n",
    "\n",
    "After running this notebook, you should be able to:\n",
    "\n",
    "- obtain, transform, and store data for use in Amazon SageMaker,\n",
    "- create an AWS SageMaker training job to produce an IP Insights model,\n",
    "- use the model to perform inference with an Amazon SageMaker endpoint.\n",
    "\n",
    "If you would like to know more, please check out the [SageMaker IP Inisghts Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/ip-insights.html).\n",
    "\n",
    "## Setup\n",
    "------\n",
    "*This notebook was created and tested on a ml.m4.xlarge notebook instance. We recommend using the same, but other instance types should still work.*\n",
    "\n",
    "The following is a cell that contains Python code.  It can be run in two ways:  \n",
    "1. Selecting the cell (click anywhere inside it), and then clicking the button above labelled \"Run\".  \n",
    "2. Selecting the cell (click anywhere inside it), and typing Shift+Return on your keyboard.  \n",
    "\n",
    "When a cell is running, you will see a star(\\*\\) in the brackets to the left (e.g., `In [*]`), and when it has completed you will see a number in the brackets. Each click of \"Run\" will execute the next cell in the notebook.\n",
    "\n",
    "Go ahead and click **Run** now. You should see the text in the `print` statement get printed just beneath the cell.\n",
    "\n",
    "All of these cells share the same interpreter, so if a cell imports modules, like this one does, those modules will be available to every subsequent cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import os\n",
    "import sagemaker\n",
    "\n",
    "print(\"Welcome to IP Insights!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTION: Configure Amazon S3 Bucket\n",
    "\n",
    "Before going further, we need to specify the S3 bucket that SageMaker will use for input and output data for the model, which will be the bucket where our training and inference tuples from CloudTrail logs and GuardDuty findings, respectively, are located. Edit the following cell to specify the name of the bucket and then run it; you do not need to change the prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'aws-reinvent2019-secml-builder-sessions'\n",
    "prefix = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the next cell to complete the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_role = sagemaker.get_execution_role()\n",
    "\n",
    "# Check if the bucket exists\n",
    "try:\n",
    "    boto3.Session().client('s3').head_bucket(Bucket=bucket)\n",
    "except botocore.exceptions.ParamValidationError as e:\n",
    "    print('Hey! You either forgot to specify your S3 bucket'\n",
    "          ' or you gave your bucket an invalid name!')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == '403':\n",
    "        print(\"Hey! You don't have permission to access the bucket, {}.\".format(bucket))\n",
    "    elif e.response['Error']['Code'] == '404':\n",
    "        print(\"Hey! Your bucket, {}, doesn't exist!\".format(bucket))\n",
    "    else:\n",
    "        raise\n",
    "else:\n",
    "    print('Training input/output will be stored in: s3://{}/{}'.format(bucket, prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Execute the two cells below to start training. Training should take several minutes to complete, and some logging information will output to the display. (These logs are also available in CloudWatch.) You can look at various training metrics in the log as the model trains.  \n",
    "When training is complete, you will see log output like this:  \n",
    ">`2019-02-11 20:34:41 Completed - Training job completed`  \n",
    ">`Billable seconds: 71`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "image = get_image_uri(boto3.Session().region_name, 'ipinsights')\n",
    "\n",
    "\n",
    "# Configure SageMaker IP Insights input channels\n",
    "train_key = os.path.join(prefix, 'train', 'cloudtrail_tuples.csv')\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_key)\n",
    "\n",
    "input_data = {\n",
    "    'train': sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', content_type='text/csv')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set up the estimator with training job configuration\n",
    "ip_insights = sagemaker.estimator.Estimator(\n",
    "    image, \n",
    "    execution_role, \n",
    "    train_instance_count=1, \n",
    "    train_instance_type='ml.m4.xlarge',\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    sagemaker_session=sagemaker.Session())\n",
    "\n",
    "# Configure algorithm-specific hyperparameters\n",
    "ip_insights.set_hyperparameters(\n",
    "    num_entity_vectors='20000',\n",
    "    random_negative_sampling_rate='5',\n",
    "    vector_dim='128', \n",
    "    mini_batch_size='1000',\n",
    "    epochs='5',\n",
    "    learning_rate='0.01',\n",
    ")\n",
    "\n",
    "# Start the training job (should take 3-4 minutes to complete)  \n",
    "ip_insights.fit(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training job name: {}'.format(ip_insights.latest_training_job.job_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Deploy Model\n",
    "Execute the cell below to deploy the trained model on an endpoint for inference. It should take 5-7 minutes to spin up the instance and deploy the model (the horizontal dashed line represents progress, and it will print an exclamation point \\[!\\] when it is complete)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW DEPLOY MODEL\n",
    "predictor = ip_insights.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOW ENDPOINT NAME\n",
    "print('Endpoint name: {}'.format(predictor.endpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Now that we have trained the model on known data, we can pass new data to it to generate scores.  We want to see if our new data looks normal, or anomalous.  \n",
    "We can pass data in a variety of formats to our inference endpoint. In this example, we will pass CSV-formmated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "predictor.content_type = 'text/csv'\n",
    "predictor.serializer = csv_serializer\n",
    "predictor.accept = 'application/json'\n",
    "predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When queried by a principal and an IPAddress, the model returns a score (called 'dot_product') which indicates how expected that event is. In other words, *the higher the dot_product, the more normal the event is.*  \n",
    "Let's first run the inference on the training (normal) data for sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Run inference on training (normal) data for sanity check\n",
    "s3_infer_data = 's3://{}/{}'.format(bucket, train_key)\n",
    "inference_data = pd.read_csv(s3_infer_data)\n",
    "print(inference_data.head())\n",
    "train_dot_products = predictor.predict(inference_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for plotting by collecting just the dot products\n",
    "train_plot_data = [x['dot_product'] for x in train_dot_products['predictions']]\n",
    "train_plot_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training data inference values as a histogram\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n, bins, patches = plt.hist(train_plot_data, 10, facecolor='blue')\n",
    "plt.xlabel('IP Insights Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice (almost) all the values above are greater than zero.\n",
    "Now let's run inference on the GuardDuty findings. Since they are from GuardDuty alerts, we expect them to be generally more anomalous, so we would expect to see lower scores..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on GuardDuty findings\n",
    "infer_key = os.path.join(prefix, 'infer', 'guardduty_tuples.csv')\n",
    "s3_infer_data = 's3://{}/{}'.format(bucket, infer_key)\n",
    "inference_data = pd.read_csv(s3_infer_data)\n",
    "print(inference_data.head())\n",
    "GuardDuty_dot_products = predictor.predict(inference_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare GuardDuty data for plotting by collecting just the dot products\n",
    "GuardDuty_plot_data = [x['dot_product'] for x in GuardDuty_dot_products['predictions']]\n",
    "GuardDuty_plot_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both the training data and the GuardDuty data together so we can compare\n",
    "\n",
    "nT, binsT, patchesT = plt.hist(GuardDuty_plot_data, 10, facecolor='red')\n",
    "nG, binsG, patchesG = plt.hist(train_plot_data, 10, facecolor='blue')\n",
    "\n",
    "plt.legend([\"GuardDuty\", \"Training\"])\n",
    "plt.xlabel('IP Insights Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! While the GuardDuty sample is small, we can see that these scores are generally lower than the scores for normal (training) data.  (Due to randomness in the training model, the precise dot product values and ranges will vary between models.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Scoring Threshold  \n",
    "It is reasonable to ask, \"What's the cutoff?\" Which inference scores should we lable \"anomalous\" and which should we label \"ok\"? We know the lowest scores are the most likely anaomalous candidates, but what about the others?  \n",
    "There is no universal threshold above which is \"normal/ok\" and below which is \"anomalous\".  Each domain and data set scores differently.  An acceptable threshold (say, 0.0) for one data set may not be appropriate for another.  \n",
    "\n",
    "So, what do we do? A common approach is to train with two types of data: data that is known \"normal\" and data that is known malicious.  If we have both, we can compare the scores of the two to find a good cut-off.  (Of course, known malicious data can be hard to come by.  In the full IPInsight tutorial, you can learn about a simple method for simulating malicious web traffic.)  \n",
    "\n",
    "A good way to see the comparison between the scores of two data sets is to plot the two distributions - both normal and anomalous - and see how they interact.  \n",
    "  \n",
    "The results of this are much easier to see with larger data sets. (We used smaller data sets above to keep the computation running times down.)\n",
    "\n",
    "**Your workshop instructor will show you example graphs drawn from larger data sets to make this concept clearer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a larget data set, it is easier to see separation between normal traffic and suspicious traffic. We could select a threshold depending on the application:\n",
    "\n",
    "    For example, if we were working with low impact decisions - such as whether to ask for another authentical factor during login - we could use a lower threshold = *<<insert appropriate value>>*. This would result in catching more true-positives, at the cost of more false-positives.\n",
    "\n",
    "    On the other hand, if our decision system were more sensitive to false positives (e.g., devoting expert analyst time to investigating suspicious activity), we could choose a higher threshold, such as threshold = *<<insert another appropriate value>>*. That way if we were sending the flagged cases to manual investigation, we would have a higher confidence that the acitivty was suspicious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would we put all this together?\n",
    "So far, you have built and endpoint, and did some analysis to determine a scoring threshold to decide which IP addresses should be tagged Anomalous.\n",
    "One way to build this into a live detection stream will be shown and discussed towards the end of the lab...\n",
    "<ol>\n",
    "<li>Use CloudWatch Events to capture specific types of GuardDuty alerts of interest.</li>\n",
    "<li> Pass the information about that GuardDuty alert to a special Lambda function you will write.\n",
    "<li>Lambda:\n",
    "    <ol>\n",
    "        <li> Takes a GuardDuty alert, finds the originating IP address from the json.\n",
    "        <li> Send that IP address to the inference endpoint you just built with your model, and get back a score.\n",
    "        <li> Compare that score to the threshold you determined for anomolous.\n",
    "        <li> Send appropriate alerts (SNS, CWE, other) if IP address is sufficiently anomolous.\n",
    "    </ol>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete the endpoint (i.e., to avoid any recurring charges)\n",
    "#sage.delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
